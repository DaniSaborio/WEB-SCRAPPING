# Plataforma de Scraping, VisualizaciÃ³n y AnÃ¡lisis LLM de Juegos

Este proyecto es una plataforma web integral diseÃ±ada para realizar scraping de datos de juegos (simulando una tienda como Steam), persistir estos datos en una base de datos PostgreSQL, visualizarlos a travÃ©s de un dashboard moderno, y utilizar un modelo de lenguaje grande (LLM) para generar anÃ¡lisis sobre el panorama del mercado de juegos. Incluye un scheduler para automatizar la ejecuciÃ³n del scraper y una gestiÃ³n robusta de logs y errores.

## ğŸŒŸ CaracterÃ­sticas Principales

* **Scraping Automatizado:** ExtracciÃ³n de datos de juegos (nombre, precios, descuentos, etc.) de una fuente web (simulada por Steam).
* **Persistencia de Datos:** Almacenamiento y gestiÃ³n de los datos extraÃ­dos en una base de datos **PostgreSQL**.
* **API RESTful:** ExposiciÃ³n de los datos recopilados y funcionalidades a travÃ©s de endpoints API en formato JSON.
* **Dashboard Moderno:** Interfaz de usuario intuitiva para visualizar los resultados del scraping, eventos y anÃ¡lisis.
* **GestiÃ³n de Archivos:** Endpoint para listar y descargar archivos CSV exportados por el scraper.
* **Scheduler Integrado:** AutomatizaciÃ³n de la ejecuciÃ³n del scraper cada 30 minutos utilizando `APScheduler`, ejecutÃ¡ndose en segundo plano con el servidor Flask.
* **AnÃ¡lisis LLM:** UtilizaciÃ³n de un Modelo de Lenguaje Grande (LLM) para obtener una opiniÃ³n y panorama general sobre los datos de juegos extraÃ­dos, accesible a travÃ©s de un endpoint API.
* **Registro de Eventos:** Sistema de logging estructurado para monitorear ejecuciones del scraper (manuales y programadas), errores y otros eventos importantes.
* **Resiliencia:** Manejo adecuado de excepciones en todo el sistema para evitar fallos en tiempo de ejecuciÃ³n y proporcionar mensajes de error claros.

## ğŸ› ï¸ TecnologÃ­as Utilizadas

* **Backend (Python 3.x):**
    * **Flask:** Microframework web para construir la API RESTful.
    * **Psycopg2:** Adaptador de PostgreSQL para Python.
    * **APScheduler:** Biblioteca para la programaciÃ³n de tareas en segundo plano.
    * **OpenAI:** Para la integraciÃ³n y consumo del Modelo de Lenguaje Grande (LLM).
    * **Selenium:** Para el scraping de contenido dinÃ¡mico de pÃ¡ginas web.
    * **BeautifulSoup4:** Para el parsing de HTML y extracciÃ³n de datos.
    * **`python-dotenv`:** Para la gestiÃ³n segura de variables de entorno.
    * **`subprocess`:** Para ejecutar el script del scraper como un proceso independiente.
* **Base de Datos:**
    * **PostgreSQL:** Sistema de gestiÃ³n de bases de datos relacionales.
* **Frontend:**
    * **HTML5, CSS3, JavaScript (Vanilla JS):** Base para la interfaz de usuario.
    * **(Opcional):** Si se utilizan librerÃ­as/frameworks CSS/JS adicionales (ej. Bootstrap, Chart.js, FullCalendar.js), listarlas aquÃ­.

## ğŸ“ Estructura del Proyecto

WEB-SCRAPPING/

â”œâ”€â”€ .env                               
â”œâ”€â”€ main.py                     
â”œâ”€â”€ requirements.txt           
â”œâ”€â”€ README.md                  
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ json_api_server.py      
â”‚   â””â”€â”€ scrapper/
â”‚       â””â”€â”€ scrapper.py        
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html             
â”‚   â”œâ”€â”€ css/                  
â”‚   â”‚   â””â”€â”€ style.css
â”‚   â””
â”‚   â”œâ”€â”€ main.js             
â”‚   â”œâ”€â”€ calendar.js         
â”‚   â”œâ”€â”€ result.js          
â”‚   â””â”€â”€ files.js            
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ llm_selector.py         
â”‚               
â”œâ”€â”€ server_data/
â”‚   â””â”€â”€ exports/                
â””â”€â”€ logs/                       

## ğŸš€ ConfiguraciÃ³n e InstalaciÃ³n

Sigue estos pasos para poner en marcha el proyecto en tu entorno local.

### 1. Clonar el Repositorio


git clone https://github.com/DaniSaborio/WEB-SCRAPPING.git
cd WEB-SCRAPPING


# Instalar dependencias

pip install -r requirements.txt

# ConfiguraciÃ³n de PostgreSQL
PG_DB=your_database_name
PG_USER=your_username
PG_PASSWORD=your_password
PG_HOST=localhost
PG_PORT=5432


ğŸŒ Endpoints de la API
La aplicaciÃ³n Flask expone los siguientes endpoints:

/ (GET): Sirve la pÃ¡gina principal del dashboard (index.html) del frontend.

/api/results (GET): Recupera y devuelve todos los resultados de juegos scrapeados desde la tabla scrapped_steam en formato JSON.

/api/events (GET): Recupera y devuelve los logs de eventos del sistema desde la tabla event_logs en formato JSON, Ãºtiles para un calendario o registro de actividad.

/api/files (GET): Lista los archivos CSV exportados disponibles en el directorio server_data/exports/. Devuelve un JSON con metadatos de los archivos (nombre, tamaÃ±o, Ãºltima modificaciÃ³n, URL de descarga).

/exports/<path:filename> (GET): Permite la descarga directa de un archivo especÃ­fico desde el directorio server_data/exports/.

Ejemplo: http://localhost:5000/exports/steam_discounts_20250725_103000.csv

/api/run_scraper (POST): Dispara manualmente la ejecuciÃ³n del script de scraping (scrapper/scrapper.py). El estado de la ejecuciÃ³n se reporta en la respuesta JSON y se loguea en event_logs.

/api/llm_analysis (GET): Obtiene un anÃ¡lisis y opiniÃ³n general del mercado de juegos, generada por el LLM, basÃ¡ndose en los datos mÃ¡s recientes scrapeados de la base de datos. Devuelve la opiniÃ³n del LLM en formato JSON.

â° AutomatizaciÃ³n (Scheduler)
El servidor Flask integra un scheduler (APScheduler) que ejecuta el script de scraping (scrapper/scrapper.py) automÃ¡ticamente cada 30 minutos en un hilo de fondo. Esto asegura que los datos estÃ©n siempre actualizados sin intervenciÃ³n manual. Los logs de estas ejecuciones programadas se registran en la tabla event_logs con el tipo "Scraper Run (Scheduled)".